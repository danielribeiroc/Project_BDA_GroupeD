{
 "paragraphs": [
  {
   "text": "import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoder}\nimport org.apache.spark.ml.classification.RandomForestClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:25:35+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "import org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.spark.ml.{Pipeline, PipelineModel}\nimport org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, OneHotEncoder}\nimport org.apache.spark.ml.classification.RandomForestClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nimport org.apache.spark.ml.tuning.{ParamGridBuilder, CrossValidator}\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717157940361_1842310479",
   "id": "paragraph_1717157940361_1842310479",
   "dateCreated": "2024-05-31T12:19:00+0000",
   "dateStarted": "2024-05-31T17:25:35+0000",
   "dateFinished": "2024-05-31T17:25:35+0000",
   "status": "FINISHED",
   "focus": true,
   "$$hashKey": "object:34779"
  },
  {
   "text": "val spark = SparkSession.builder()\n  .appName(\"Football Data Analysis\")\n  .getOrCreate()\n  \n// Specify the path to the directory containing the JSON files\nval bucketPath = \"gs://2024-bda-projet-group-d/open-data/data/events/*.json\"\n\n// Read all JSON files in the directory\nval eventsDF = spark.read\n  .option(\"header\", \"true\")\n  .option(\"multiline\", \"true\")\n  .json(bucketPath)\n\n// Show the first 5 rows of the DataFrame\neventsDF.show(5)",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:25:35+0000",
   "progress": 100,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+-----+-------------+------------+-------------+-----+-----+---------+------------+-------+----+--------+--------------+--------+----------+--------+----------+--------------------+-----+---------------+------------+------------+------+----------+----------+----+--------------------+------+------------------+-------------------+----------+--------------------+----------+---------------+--------------------+------+----+------------+--------------------+--------------+------------+-----------------+--------------+\n|50_50|bad_behaviour|ball_receipt|ball_recovery|block|carry|clearance|counterpress|dribble|duel|duration|foul_committed|foul_won|goalkeeper|half_end|half_start|                  id|index|injury_stoppage|interception|    location|minute|miscontrol|off_camera| out|                pass|period|      play_pattern|             player|player_off|            position|possession|possession_team|      related_events|second|shot|substitution|             tactics|          team|   timestamp|             type|under_pressure|\n+-----+-------------+------------+-------------+-----+-----+---------+------------+-------+----+--------+--------------+--------+----------+--------+----------+--------------------+-----+---------------+------------+------------+------+----------+----------+----+--------------------+------+------------------+-------------------+----------+--------------------+----------+---------------+--------------------+------+----+------------+--------------------+--------------+------------+-----------------+--------------+\n| NULL|         NULL|        NULL|         NULL| NULL| NULL|     NULL|        NULL|   NULL|NULL|     0.0|          NULL|    NULL|      NULL|    NULL|      NULL|85301abf-234c-4fa...|    1|           NULL|        NULL|        NULL|     0|      NULL|      NULL|NULL|                NULL|     1| {1, Regular Play}|               NULL|      NULL|                NULL|         1| {785, Croatia}|                NULL|     0|NULL|        NULL|{433, [{1, {16531...|{785, Croatia}|00:00:00.000|{35, Starting XI}|          NULL|\n| NULL|         NULL|        NULL|         NULL| NULL| NULL|     NULL|        NULL|   NULL|NULL|     0.0|          NULL|    NULL|      NULL|    NULL|      NULL|b859d5b8-fdc2-409...|    2|           NULL|        NULL|        NULL|     0|      NULL|      NULL|NULL|                NULL|     1| {1, Regular Play}|               NULL|      NULL|                NULL|         1| {785, Croatia}|                NULL|     0|NULL|        NULL|{4411, [{1, {5547...| {781, Brazil}|00:00:00.000|{35, Starting XI}|          NULL|\n| NULL|         NULL|        NULL|         NULL| NULL| NULL|     NULL|        NULL|   NULL|NULL|     0.0|          NULL|    NULL|      NULL|    NULL|      NULL|1263eb64-ec56-4b2...|    3|           NULL|        NULL|        NULL|     0|      NULL|      NULL|NULL|                NULL|     1| {1, Regular Play}|               NULL|      NULL|                NULL|         1| {785, Croatia}|[03eed776-a228-40...|     0|NULL|        NULL|                NULL|{785, Croatia}|00:00:00.000| {18, Half Start}|          NULL|\n| NULL|         NULL|        NULL|         NULL| NULL| NULL|     NULL|        NULL|   NULL|NULL|     0.0|          NULL|    NULL|      NULL|    NULL|      NULL|03eed776-a228-408...|    4|           NULL|        NULL|        NULL|     0|      NULL|      NULL|NULL|                NULL|     1| {1, Regular Play}|               NULL|      NULL|                NULL|         1| {785, Croatia}|[1263eb64-ec56-4b...|     0|NULL|        NULL|                NULL| {781, Brazil}|00:00:00.000| {18, Half Start}|          NULL|\n| NULL|         NULL|        NULL|         NULL| NULL| NULL|     NULL|        NULL|   NULL|NULL| 0.86554|          NULL|    NULL|      NULL|    NULL|      NULL|9f66f598-c069-4be...|    5|           NULL|        NULL|[60.0, 40.0]|     0|      NULL|      NULL|NULL|{NULL, -3.038068,...|     1|{9, From Kick Off}|{5463, Luka Modrić}|      NULL|{13, Right Center...|         2| {785, Croatia}|[687facae-c228-45...|     0|NULL|        NULL|                NULL|{785, Croatia}|00:00:00.489|       {30, Pass}|          NULL|\n+-----+-------------+------------+-------------+-----+-----+---------+------------+-------+----+--------+--------------+--------+----------+--------+----------+--------------------+-----+---------------+------------+------------+------+----------+----------+----+--------------------+------+------------------+-------------------+----------+--------------------+----------+---------------+--------------------+------+----+------------+--------------------+--------------+------------+-----------------+--------------+\nonly showing top 5 rows\n\n\u001b[1m\u001b[34mspark\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.SparkSession\u001b[0m = org.apache.spark.sql.SparkSession@27b6bd07\n\u001b[1m\u001b[34mbucketPath\u001b[0m: \u001b[1m\u001b[32mString\u001b[0m = gs://2024-bda-projet-group-d/open-data/data/events/*.json\n\u001b[1m\u001b[34meventsDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [50_50: struct<outcome: struct<id: bigint, name: string>>, bad_behaviour: struct<card: struct<id: bigint, name: string>> ... 40 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=0",
       "$$hashKey": "object:35183"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=1",
       "$$hashKey": "object:35184"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=2",
       "$$hashKey": "object:35185"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786443_1513317973",
   "id": "paragraph_1717053394427_1873686067",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:25:35+0000",
   "dateFinished": "2024-05-31T17:28:36+0000",
   "status": "FINISHED",
   "$$hashKey": "object:34780"
  },
  {
   "text": "val typeDF = eventsDF.select(col(\"type.id\").alias(\"type\"))\n\n// Show the first 5 rows of the new DataFrame\ntypeDF.show(5)",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:28:36+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+----+\n|type|\n+----+\n|  35|\n|  35|\n|  18|\n|  18|\n|  30|\n+----+\nonly showing top 5 rows\n\n\u001b[1m\u001b[34mtypeDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [type: bigint]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=3",
       "$$hashKey": "object:35251"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_1230187613",
   "id": "paragraph_1717054060732_868004925",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:28:36+0000",
   "dateFinished": "2024-05-31T17:28:39+0000",
   "status": "FINISHED",
   "$$hashKey": "object:34781"
  },
  {
   "text": "// Define the window specification\nval windowSpec = Window.rowsBetween(-19, 0)\n\n// Apply the window function to create the sliding window\nval slidingWindowDF = typeDF\n  .withColumn(\"events_window\", collect_list(\"type\").over(windowSpec))\n",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:28:39+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mwindowSpec\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.expressions.WindowSpec\u001b[0m = org.apache.spark.sql.expressions.WindowSpec@2cf0928d\n\u001b[1m\u001b[34mslidingWindowDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [type: bigint, events_window: array<bigint>]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_297652721",
   "id": "paragraph_1717137303524_1709748586",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:28:40+0000",
   "dateFinished": "2024-05-31T17:28:41+0000",
   "status": "FINISHED",
   "$$hashKey": "object:34782"
  },
  {
   "text": "// Show the first 5 rows of the new DataFrame with the sliding window\nslidingWindowDF.show(10, false)",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:28:41+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+----+----------------------------------------+\n|type|events_window                           |\n+----+----------------------------------------+\n|35  |[35]                                    |\n|35  |[35, 35]                                |\n|18  |[35, 35, 18]                            |\n|18  |[35, 35, 18, 18]                        |\n|30  |[35, 35, 18, 18, 30]                    |\n|42  |[35, 35, 18, 18, 30, 42]                |\n|43  |[35, 35, 18, 18, 30, 42, 43]            |\n|30  |[35, 35, 18, 18, 30, 42, 43, 30]        |\n|42  |[35, 35, 18, 18, 30, 42, 43, 30, 42]    |\n|43  |[35, 35, 18, 18, 30, 42, 43, 30, 42, 43]|\n+----+----------------------------------------+\nonly showing top 10 rows\n\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=4",
       "$$hashKey": "object:35361"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=5",
       "$$hashKey": "object:35362"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_1192522595",
   "id": "paragraph_1717137462426_35282807",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:28:41+0000",
   "dateFinished": "2024-05-31T17:30:00+0000",
   "status": "FINISHED",
   "$$hashKey": "object:34783"
  },
  {
   "text": "val filteredDF = slidingWindowDF\n  .filter(size(col(\"events_window\")) === 20)",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:30:00+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mfilteredDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.Dataset[org.apache.spark.sql.Row]\u001b[0m = [type: bigint, events_window: array<bigint>]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_1616910998",
   "id": "paragraph_1717139229374_1292929337",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:30:01+0000",
   "dateFinished": "2024-05-31T17:30:01+0000",
   "status": "FINISHED",
   "$$hashKey": "object:34784"
  },
  {
   "text": "val expandedDF = filteredDF\n  .withColumn(\"event1\", col(\"events_window\").getItem(0))\n  .withColumn(\"event2\", col(\"events_window\").getItem(1))\n  .withColumn(\"event3\", col(\"events_window\").getItem(2))\n  .withColumn(\"event4\", col(\"events_window\").getItem(3))\n  .withColumn(\"event5\", col(\"events_window\").getItem(4))\n  .withColumn(\"event6\", col(\"events_window\").getItem(5))\n  .withColumn(\"event7\", col(\"events_window\").getItem(6))\n  .withColumn(\"event8\", col(\"events_window\").getItem(7))\n  .withColumn(\"event9\", col(\"events_window\").getItem(8))\n  .withColumn(\"event10\", col(\"events_window\").getItem(9))\n  .withColumn(\"event11\", col(\"events_window\").getItem(10))\n  .withColumn(\"event12\", col(\"events_window\").getItem(11))\n  .withColumn(\"event13\", col(\"events_window\").getItem(12))\n  .withColumn(\"event14\", col(\"events_window\").getItem(13))\n  .withColumn(\"event15\", col(\"events_window\").getItem(14))\n  .withColumn(\"event16\", col(\"events_window\").getItem(15))\n  .withColumn(\"event17\", col(\"events_window\").getItem(16))\n  .withColumn(\"event18\", col(\"events_window\").getItem(17))\n  .withColumn(\"event19\", col(\"events_window\").getItem(18))\n  .withColumn(\"event20\", col(\"events_window\").getItem(19))\n\n  .drop(\"type\", \"events_window\")\n \nexpandedDF.show(5)",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:30:01+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n|event1|event2|event3|event4|event5|event6|event7|event8|event9|event10|event11|event12|event13|event14|event15|event16|event17|event18|event19|event20|\n+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\n|    35|    35|    18|    18|    30|    42|    43|    30|    42|     43|     30|     42|     43|     30|     42|     43|     30|     42|     43|     30|\n|    35|    18|    18|    30|    42|    43|    30|    42|    43|     30|     42|     43|     30|     42|     43|     30|     42|     43|     30|     42|\n|    18|    18|    30|    42|    43|    30|    42|    43|    30|     42|     43|     30|     42|     43|     30|     42|     43|     30|     42|     43|\n|    18|    30|    42|    43|    30|    42|    43|    30|    42|     43|     30|     42|     43|     30|     42|     43|     30|     42|     43|     30|\n|    30|    42|    43|    30|    42|    43|    30|    42|    43|     30|     42|     43|     30|     42|     43|     30|     42|     43|     30|     42|\n+------+------+------+------+------+------+------+------+------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+-------+\nonly showing top 5 rows\n\n\u001b[1m\u001b[34mexpandedDF\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.DataFrame\u001b[0m = [event1: bigint, event2: bigint ... 18 more fields]\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=6",
       "$$hashKey": "object:35476"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=7",
       "$$hashKey": "object:35477"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_1662122375",
   "id": "paragraph_1717137576409_1065319230",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:30:01+0000",
   "dateFinished": "2024-05-31T17:31:17+0000",
   "status": "FINISHED",
   "$$hashKey": "object:34785"
  },
  {
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "status": "READY",
   "text": "// Show size in GB of expandedDF\nprintln(expandedDF.count() + \" rows\")",
   "id": "",
   "dateCreated": "2024-06-03 11:38:43.751",
   "config": {}
  },
  {
   "text": "// Définition du schéma pour le DataFrame --> utile pour définir quel features ont veut utiliser lors de l'entrainement\nval schema = new StructType()\n  // Ajout des colonnes avec leur nom, type et utilisation ou non\n  .add(\"event1\", LongType, true)\n  .add(\"event2\", LongType, true)\n  .add(\"event3\", LongType, true)\n  .add(\"event4\", LongType, true)\n  .add(\"event5\", LongType, true)\n  .add(\"event6\", LongType, false)\n  .add(\"event7\", LongType, false)\n  .add(\"event8\", LongType, false)\n  .add(\"event9\", LongType, false)\n  .add(\"event10\", LongType, false)\n  .add(\"event11\", LongType, false)\n  .add(\"event12\", LongType, false)\n  .add(\"event13\", LongType, false)\n  .add(\"event14\", LongType, false)\n  .add(\"event15\", LongType, false)\n  .add(\"event16\", LongType, false)\n  .add(\"event17\", LongType, false)\n  .add(\"event18\", LongType, false)\n  .add(\"event19\", LongType, false)\n  .add(\"event20\", LongType, false)",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:31:17+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "SUCCESS",
    "msg": [
     {
      "type": "TEXT",
      "data": "\u001b[1m\u001b[34mschema\u001b[0m: \u001b[1m\u001b[32morg.apache.spark.sql.types.StructType\u001b[0m = StructType(StructField(event1,LongType,true),StructField(event2,LongType,true),StructField(event3,LongType,true),StructField(event4,LongType,true),StructField(event5,LongType,true),StructField(event6,LongType,false),StructField(event7,LongType,false),StructField(event8,LongType,false),StructField(event9,LongType,false),StructField(event10,LongType,false),StructField(event11,LongType,false),StructField(event12,LongType,false),StructField(event13,LongType,false),StructField(event14,LongType,false),StructField(event15,LongType,false),StructField(event16,LongType,false),StructField(event17,LongType,false),StructField(event18,LongType,false),StructField(event19,LongType,false),StructField(event20,LongType,false))\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_1697525121",
   "id": "paragraph_1717139186651_286996340",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:31:17+0000",
   "dateFinished": "2024-05-31T17:31:17+0000",
   "status": "FINISHED",
   "$$hashKey": "object:34786"
  },
  {
   "text": "// 1. Définir les colonnes de features et assembler les features en un vecteur\n\nval featureCols = Array(\"event1\", \"event2\", \"event3\", \"event4\", \"event5\") //, \"event6\", \"event7\", \"event8\", \"event9\", \"event10\", \"event11\", \"event12\", \"event13\", \"event14\", \"event15\", \"event16\", \"event17\", \"event18\", \"event19\", \"event20\")\n\nval assembler = new VectorAssembler()\n  .setInputCols(featureCols)\n  .setOutputCol(\"features\")\n\n// Repartition the DataFrame to distribute data across the cluster\nval numPartitions = 200  // Adjust based on your cluster size and data\nval repartitionedDF = expandedDF.repartition(numPartitions)\n\n// Cache the DataFrame to avoid recomputation\nrepartitionedDF.cache()\n\n\n// 3. Split les données en set de train et de test\nval Array(trainingData, testData) = repartitionedDF.randomSplit(Array(0.8, 0.2))\n\n// 4. Définir les modèles de classification\nval rf = new RandomForestClassifier()\n  .setLabelCol(\"event6\")\n  .setFeaturesCol(\"features\")\n\n// 6. Définir l'évaluateur pour mesurer la précision ou autres\nval evaluator = new MulticlassClassificationEvaluator()\n  .setLabelCol(\"event6\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"accuracy\")\n\n// 7. Configurer la cross validation\nval rfCv = new CrossValidator() // --> random forest\n  .setEstimator(rf)\n  .setEvaluator(evaluator)\n  .setNumFolds(3)\n\n// 8. Création des pipelines pour chaque modèle (comme dans la documentation)\nval rfPipeline = new Pipeline().setStages(Array(assembler, rfCv)) // --> random forest\n\n// Train\nval rfModel = rfPipeline.fit(trainingData) // --> random forest\n\n// Faire les prédictions\nval rfPredictions = rfModel.transform(testData) // --> random forest\n\n// Evaluer les modèles\nval rfAccuracy = evaluator.evaluate(rfPredictions) // --> random forest\n\nprintln(s\"Random Forest Test set accuracy = $rfAccuracy\")",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T17:31:17+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "org.apache.spark.SparkException: Exception thrown in awaitResult:\n  at org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:56)\n  at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:310)\n  at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$9(CrossValidator.scala:186)\n  at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$9$adapted(CrossValidator.scala:186)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n  at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$4(CrossValidator.scala:186)\n  at scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n  at scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n  at scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n  at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n  at scala.collection.TraversableLike.map(TraversableLike.scala:286)\n  at scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n  at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n  at org.apache.spark.ml.tuning.CrossValidator.$anonfun$fit$1(CrossValidator.scala:166)\n  at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n  at scala.util.Try$.apply(Try.scala:213)\n  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n  at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:137)\n  at org.apache.spark.ml.tuning.CrossValidator.fit(CrossValidator.scala:80)\n  at org.apache.spark.ml.Pipeline.$anonfun$fit$5(Pipeline.scala:151)\n  at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)\n  at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)\n  at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)\n  at org.apache.spark.ml.Pipeline.$anonfun$fit$4(Pipeline.scala:151)\n  at scala.collection.Iterator.foreach(Iterator.scala:943)\n  at scala.collection.Iterator.foreach$(Iterator.scala:943)\n  at scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\n  at org.apache.spark.ml.Pipeline.$anonfun$fit$2(Pipeline.scala:147)\n  at org.apache.spark.ml.MLEvents.withFitEvent(events.scala:130)\n  at org.apache.spark.ml.MLEvents.withFitEvent$(events.scala:123)\n  at org.apache.spark.ml.util.Instrumentation.withFitEvent(Instrumentation.scala:42)\n  at org.apache.spark.ml.Pipeline.$anonfun$fit$1(Pipeline.scala:133)\n  at org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\n  at scala.util.Try$.apply(Try.scala:213)\n  at org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\n  at org.apache.spark.ml.Pipeline.fit(Pipeline.scala:133)\n  ... 56 elided\nCaused by: org.apache.spark.SparkException: Job 68 cancelled because SparkContext was shut down\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1(DAGScheduler.scala:1248)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$cleanUpAfterSchedulerStop$1$adapted(DAGScheduler.scala:1246)\n  at scala.collection.mutable.HashSet.foreach(HashSet.scala:79)\n  at org.apache.spark.scheduler.DAGScheduler.cleanUpAfterSchedulerStop(DAGScheduler.scala:1246)\n  at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onStop(DAGScheduler.scala:3075)\n  at org.apache.spark.util.EventLoop.stop(EventLoop.scala:84)\n  at org.apache.spark.scheduler.DAGScheduler.$anonfun$stop$3(DAGScheduler.scala:2961)\n  at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1376)\n  at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:2961)\n  at org.apache.spark.SparkContext.$anonfun$stop$12(SparkContext.scala:2280)\n  at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1376)\n  at org.apache.spark.SparkContext.stop(SparkContext.scala:2280)\n  at org.apache.spark.SparkContext.stop(SparkContext.scala:2233)\n  at org.apache.spark.SparkContext.$anonfun$new$35(SparkContext.scala:703)\n  at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:214)\n  at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$2(ShutdownHookManager.scala:188)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1929)\n  at org.apache.spark.util.SparkShutdownHookManager.$anonfun$runAll$1(ShutdownHookManager.scala:188)\n  at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n  at scala.util.Try$.apply(Try.scala:213)\n  at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:188)\n  at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:178)\n  at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)\n  at java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n  ... 3 more\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {
    "jobUrl": {
     "propertyName": "jobUrl",
     "label": "SPARK JOB",
     "tooltip": "View in Spark web UI",
     "group": "spark",
     "values": [
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=8",
       "$$hashKey": "object:35591"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=9",
       "$$hashKey": "object:35592"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=10",
       "$$hashKey": "object:35593"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=11",
       "$$hashKey": "object:35594"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=12",
       "$$hashKey": "object:35595"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=13",
       "$$hashKey": "object:35596"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=14",
       "$$hashKey": "object:35597"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=15",
       "$$hashKey": "object:35598"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=16",
       "$$hashKey": "object:35599"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=17",
       "$$hashKey": "object:35600"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=18",
       "$$hashKey": "object:35601"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=19",
       "$$hashKey": "object:35602"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=20",
       "$$hashKey": "object:35603"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=21",
       "$$hashKey": "object:35604"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=22",
       "$$hashKey": "object:35605"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=23",
       "$$hashKey": "object:35606"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=24",
       "$$hashKey": "object:35607"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=25",
       "$$hashKey": "object:35608"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=26",
       "$$hashKey": "object:35609"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=27",
       "$$hashKey": "object:35610"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=28",
       "$$hashKey": "object:35611"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=29",
       "$$hashKey": "object:35612"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=30",
       "$$hashKey": "object:35613"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=31",
       "$$hashKey": "object:35614"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=32",
       "$$hashKey": "object:35615"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=33",
       "$$hashKey": "object:35616"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=34",
       "$$hashKey": "object:35617"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=35",
       "$$hashKey": "object:35618"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=36",
       "$$hashKey": "object:35619"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=37",
       "$$hashKey": "object:35620"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=38",
       "$$hashKey": "object:35621"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=39",
       "$$hashKey": "object:35622"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=40",
       "$$hashKey": "object:35623"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=41",
       "$$hashKey": "object:35624"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=42",
       "$$hashKey": "object:35625"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=43",
       "$$hashKey": "object:35626"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=44",
       "$$hashKey": "object:35627"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=45",
       "$$hashKey": "object:35628"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=46",
       "$$hashKey": "object:35629"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=47",
       "$$hashKey": "object:35630"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=48",
       "$$hashKey": "object:35631"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=49",
       "$$hashKey": "object:35632"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=50",
       "$$hashKey": "object:35633"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=51",
       "$$hashKey": "object:35634"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=52",
       "$$hashKey": "object:35635"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=53",
       "$$hashKey": "object:35636"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=54",
       "$$hashKey": "object:35637"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=55",
       "$$hashKey": "object:35638"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=56",
       "$$hashKey": "object:35639"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=57",
       "$$hashKey": "object:35640"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=58",
       "$$hashKey": "object:35641"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=59",
       "$$hashKey": "object:35642"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=60",
       "$$hashKey": "object:35643"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=61",
       "$$hashKey": "object:35644"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=62",
       "$$hashKey": "object:35645"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=63",
       "$$hashKey": "object:35646"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=64",
       "$$hashKey": "object:35647"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=65",
       "$$hashKey": "object:35648"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=66",
       "$$hashKey": "object:35649"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=67",
       "$$hashKey": "object:35650"
      },
      {
       "jobUrl": "http://cluster-e53a-m.europe-west6-a.c.bigdatasemestre2.internal:42019/jobs/job?id=68",
       "$$hashKey": "object:35651"
      }
     ],
     "interpreterSettingId": "spark"
    }
   },
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_1431880191",
   "id": "paragraph_1717139737688_1095583986",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T17:31:17+0000",
   "dateFinished": "2024-05-31T18:05:19+0000",
   "status": "ERROR",
   "$$hashKey": "object:34787"
  },
  {
   "text": "rfPredictions.select(\"event6\", \"prediction\").show(10)",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T11:03:06+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "results": {
    "code": "ERROR",
    "msg": [
     {
      "type": "TEXT",
      "data": "org.apache.zeppelin.interpreter.InterpreterException: org.apache.zeppelin.interpreter.InterpreterException: Fail to open SparkInterpreter\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:76)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:844)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:752)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:172)\n\tat org.apache.zeppelin.scheduler.AbstractScheduler.runJob(AbstractScheduler.java:132)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler.lambda$runJobInScheduler$0(FIFOScheduler.java:42)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.zeppelin.interpreter.InterpreterException: Fail to open SparkInterpreter\n\tat org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:138)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:70)\n\t... 8 more\nCaused by: java.lang.reflect.InvocationTargetException\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.spark2CreateContext(BaseSparkScalaInterpreter.scala:290)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.createSparkContext(BaseSparkScalaInterpreter.scala:228)\n\tat org.apache.zeppelin.spark.SparkScala212Interpreter.open(SparkScala212Interpreter.scala:88)\n\tat org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:122)\n\t... 9 more\nCaused by: java.lang.IllegalStateException: Shutdown hooks cannot be modified during shutdown.\n\tat org.apache.spark.util.SparkShutdownHookManager.add(ShutdownHookManager.scala:195)\n\tat org.apache.spark.util.ShutdownHookManager$.addShutdownHook(ShutdownHookManager.scala:153)\n\tat org.apache.spark.storage.DiskBlockManager.addShutdownHook(DiskBlockManager.scala:344)\n\tat org.apache.spark.storage.DiskBlockManager.<init>(DiskBlockManager.scala:79)\n\tat org.apache.spark.storage.BlockManager.<init>(BlockManager.scala:204)\n\tat org.apache.spark.SparkEnv$.create(SparkEnv.scala:381)\n\tat org.apache.spark.SparkEnv$.createDriverEnv(SparkEnv.scala:196)\n\tat org.apache.spark.SparkContext.createSparkEnv(SparkContext.scala:283)\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:485)\n\tat org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2922)\n\tat org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:1099)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:1093)\n\t... 17 more\n"
     }
    ]
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_219486544",
   "id": "paragraph_1717144308475_1163562688",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "dateStarted": "2024-05-31T11:03:16+0000",
   "dateFinished": "2024-05-31T11:42:30+0000",
   "status": "ERROR",
   "$$hashKey": "object:34788"
  },
  {
   "text": "",
   "user": "anonymous",
   "dateUpdated": "2024-05-31T11:15:38+0000",
   "progress": 0,
   "config": {
    "editorSetting": {
     "language": "scala",
     "editOnDblClick": false,
     "completionKey": "TAB",
     "completionSupport": true
    },
    "colWidth": 12,
    "editorMode": "ace/mode/scala",
    "fontSize": 9,
    "editorHide": false,
    "results": {},
    "enabled": true
   },
   "settings": {
    "params": {},
    "forms": {}
   },
   "apps": [],
   "runtimeInfos": {},
   "progressUpdateIntervalMs": 500,
   "jobName": "paragraph_1717152786444_142451985",
   "id": "paragraph_1717140585210_855311603",
   "dateCreated": "2024-05-31T10:53:06+0000",
   "status": "READY",
   "$$hashKey": "object:34789"
  }
 ],
 "name": "Test",
 "id": "2JXNY4AKS",
 "defaultInterpreterGroup": "spark",
 "version": "0.10.1",
 "noteParams": {},
 "noteForms": {},
 "angularObjects": {},
 "config": {
  "isZeppelinNotebookCronEnable": false,
  "looknfeel": "default",
  "personalizedMode": "false"
 },
 "info": {
  "isRunning": false
 },
 "path": "/Test"
}